# MVP Multi‑Agent LLM System Aligned with SPARC Framework

## Introduction and SPARC Framework Overview

The **SPARC framework** is an open methodology for structured AI-assisted software development, emphasizing multi-agent collaboration and stepwise refinement. SPARC is an acronym for the phases **Specification, Pseudocode, Architecture, Refinement, Completion**, which mirror an agile development lifecycle. In practice, SPARC means breaking down a complex goal into these stages, assigning specialized *agent roles* at each stage to ensure thorough analysis, planning, implementation, and review. Each agent operates under carefully engineered prompts and protocols, forming a “structured AI team” that dramatically improves performance over naive single-prompt approaches. Key principles include using **structured templates** for consistency, **specialized roles** for focused expertise, a **boomerang routing** pattern to hand tasks from a planner to specialists and back, and **feedback loops** for continuous improvement.

Our goal is to implement a self-hosted **MVP (Minimum Viable Product)** that fully aligns with SPARC’s design philosophy – i.e. robust scaffolding of tasks, advanced prompt engineering per role, multi-agent orchestration, intelligent routing of subtasks, and built-in critique. The system will be developed in Python and use **Ray** for distributed agent orchestration, enabling parallel and scalable execution of agent tasks. We will incorporate all core agent roles defined by SPARC and related literature (Planner, Researcher, Coder, QA, Critic, and Judge) and integrate them with local LLM models and observability tools. The end result will be a containerized codebase with deployment templates for both Docker Compose and Kubernetes (via K3s/Helm), ready to run **locally** while also capable of iterative self-improvement.

## Core Agent Roles and Responsibilities

To mirror SPARC’s multi-agent team, the system defines **six core agents**, each with a distinct role and prompt schema. This specialization follows the *Single Responsibility Principle* – each agent focuses on what it does best – and together they collaborate to tackle complex tasks. Below are the agent roles and their responsibilities:

* **Planner (Orchestrator)** – Breaks down high-level objectives into concrete tasks and assigns them to other agents. The Planner analyzes the problem, *decomposes* it into sub-tasks, identifies dependencies, and decides which specialist agent should handle each part. It provides each specialist with a clear task definition, relevant context, and expected output format. The Planner also receives results from agents and verifies that requirements are met. Prompting for the Planner includes guidelines to *“create clear, unambiguous task assignments”* with defined scope, dependencies, and deliverables. This agent essentially coordinates the workflow (like a project manager), maintaining the “big picture” and ensuring all pieces fit together.

* **Researcher (Information Specialist)** – Responsible for knowledge gathering, data collection, and evidence synthesis. When a task requires external information or analysis of documentation, the Planner delegates it to the Researcher. This agent might perform web searches, read project documentation, or query knowledge bases. The Researcher’s prompt emphasizes *credible sources, cross-verification,* and detailed citation of facts. For example, it might be instructed to *“focus on authoritative sources, triangulate information across multiple references, and document all sources with proper citations.”*. The Researcher returns a structured report of findings (with references) back to the Planner or directly to a Coder agent that needs the information.

* **Coder (Developer)** – Implements solutions by writing code or generating other structured outputs (e.g. configuration files) based on specifications. The Coder agent receives a detailed task (often with pseudocode or explicit requirements from the Planner) and produces the code to fulfill it. This agent follows best practices for coding (style, correctness) and may use tools for code generation and debugging. In refinement phases, the Coder may repeatedly improve the code in response to feedback from QA or Critic agents. The Coder’s prompt would include instructions to adhere to any architectural constraints and to produce well-documented, efficient code. In alignment with SPARC’s *Refinement* stage, the Coder works iteratively – implementing features, then optimizing or adjusting as needed. We also integrate a secure *code interpreter* environment for the Coder to execute or test code (using something like E2B sandbox as in SPARC 2.0), so it can verify its output before handing off.

* **QA (Tester/Validator)** – Ensures quality and correctness of outputs, particularly code. The QA agent runs tests, checks edge cases, and validates that the Coder’s output meets the acceptance criteria. For code tasks, QA might execute unit tests or static analysis; for text answers, QA could verify factual accuracy or consistency. This agent can use various tools (compilers, test suites, linters, etc.) to evaluate the work. The QA agent’s prompt focuses on *verification standards* – e.g. *“ensure adherence to requirements, correctness, and no security policy violations”*. In practice, this could encompass a **Security Validator** function as well (checking outputs against policies). If QA finds issues, it reports them (possibly with suggestions) to the Planner or directly back to the Coder. This feedback loop contributes to fault tolerance: if an agent’s output isn’t good, another agent (QA or Critic) flags it and the system can retry or refine rather than failing outright.

* **Critic (Reviewer)** – Provides a critical evaluation of intermediate or final results and suggests improvements. The Critic is like a code reviewer or an editor: it examines the work done by the Coder (or even the plan from the Planner) to identify flaws, potential optimizations, or unmet requirements. The Critic’s prompt is geared toward *analytical assessment* and constructive feedback. For instance, after the Coder agent produces code, the Critic agent might review it for efficiency, readability, or alignment with the architectural design, pointing out any bugs or suboptimal approaches. In a *debate* style use, the Critic could also generate an alternative solution or highlight why the current solution might fail, essentially acting as an adversarial tester. This concept aligns with research on using one agent to evaluate another agent’s output, which has been shown to yield valuable step-by-step feedback. The Critic’s feedback is fed either to the Coder for another refinement pass or to the Planner to reconsider the approach.

* **Judge (Evaluator/Decider)** – Makes final decisions or approvals on the task completion. The Judge agent can be seen as a special evaluator that takes the outputs (and possibly the Critic’s notes or QA results) and determines if the task’s objectives have been met satisfactorily. If yes, the Judge “green-lights” the completion; if not, it may coordinate another iteration. In some designs, the Planner might act as the judge, but here we separate it for clarity. The Judge’s prompt would include criteria for success (correctness, efficiency, compliance, etc.) and it may weigh multiple candidate solutions if available. Essentially, this agent implements the *Agent-as-a-Judge* idea – using an AI agent to systematically evaluate other agents’ work. Studies have found that such AI judges can align closely with human evaluations and drastically reduce the need for human review. In our system, once the Judge approves, the task is considered complete and results can be deployed or returned to the user. If the Judge is not satisfied, it can trigger a feedback loop (perhaps instructing the Critic to propose fixes or asking the Planner to modify the plan). This ensures a high reliability bar for the final output, enabling **autonomous self-correction** until quality standards are met.

All agents communicate through structured messages and adhere to a shared “team protocol.” For example, the Planner (Orchestrator) will format task assignments in a specific template that includes sections like *Context, Task Definition, Expected Output, Return Instructions,* and metadata (task IDs, etc.). Specialists return their results in a similarly structured format with sections for deliverables, issues encountered, and next-step recommendations. This structured hand-off (the “boomerang” pattern) ensures the Planner (or Judge) has all the info needed to integrate the results. It also facilitates the memory system logging (as these sections can be parsed and stored). The net effect is a **team of LLM agents** working in concert, each one an expert in its role, communicating clearly, and catching each other’s errors – akin to a human project team of analysts, developers, testers, and managers.

## Orchestration with Ray: Task Routing and Parallelism

To coordinate these agents efficiently, we leverage **Ray**, a distributed computing framework that adds parallelism and scaling to Python applications. Ray allows us to define each agent as a *remote actor* (or its actions as remote tasks) that can run concurrently on separate processes or machines. Using Ray’s task graph capabilities, we can express the workflow as a **DAG (Directed Acyclic Graph)** of tasks where outputs of one agent feed into the next. This is a natural fit for the SPARC pipeline where, for example, the Planner’s output triggers the Researcher and Coder in parallel, and their results later flow into QA and Critic tasks before final aggregation.

Ray’s computation graph API makes it straightforward to declare these dependencies. For instance, the Planner can spawn multiple agent tasks and use Ray’s futures to wait for their completion. We might have something like:

```python
planner_result = planner_agent.run.remote(user_request)
research_report = researcher_agent.run.remote(planner_result)
code_draft = coder_agent.run.remote(planner_result)
# Wait for research and coding to finish
draft, report = ray.get([code_draft, research_report])
qa_feedback = qa_agent.run.remote(draft, tests=test_suite)
critic_review = critic_agent.run.remote(draft, context=report)
# Gather QA and Critic feedback
feedback, critique = ray.get([qa_feedback, critic_review])
final_decision = judge_agent.run.remote(draft, feedback, critique)
result = ray.get(final_decision)
```

This is a simplified illustration – in practice we would include more robust routing logic – but it shows how Ray can handle multiple agents and their interactions. We can also exploit **parallelism**: for independent sub-tasks (say the user’s request can be split into two unrelated coding tasks), the Planner could dispatch two Coder agents simultaneously and Ray will execute them in parallel given available resources. Ray’s scheduler will manage the CPU/GPU allocation and ensure tasks run to completion, even if distributed across a cluster.

Using Ray for an agent system has several benefits. It is not specific to LLMs, but it naturally fits the model of breaking a job into pieces and running them concurrently or sequentially as needed. Ray’s actors maintain state, which is useful for agents that accumulate memory or context during a session. The *Ray Workflows* API can even provide durability and stateful management of multi-step workflows, which could be leveraged for long-running tasks that need recovery or for implementing the self-improvement loops (with checkpoints) in our system. Overall, Ray gives us a flexible orchestration layer: **agents as distributed services** that can be scaled and pipelined with minimal overhead. As one source notes, *“Ray’s execution DAGs are a great solution for running agentic workflows, naturally supporting parallel execution and batching.”*

**Task Routing** within this architecture is primarily handled by the Planner (Orchestrator) and to some extent the Judge. The Planner decides *which* agents to invoke for a given task and provides them the necessary input. In complex scenarios, we might implement a dynamic router: for example, if a subtask is recognized as a “research” problem, it goes to the Researcher; if it’s a pure Q\&A, perhaps it can go directly to an LLM Q\&A agent; if it’s code, to the Coder, etc. This decision can be made via simple rules or learned policies. We will start with a **rule-based routing** (mapping task types to agents) derived from the SPARC methodology (e.g., tasks coming out of Specification likely route to Researcher or directly to Pseudocode drafting by Planner itself; tasks from Pseudocode to Coder; outputs from Coder always route to QA and Critic; and so on). The routing table itself can be part of a *manifest* (a config file describing the workflow, which the system can even adjust over time).

After an agent finishes its work, the designated *boomerang\_return\_to* field ensures the output goes back to the correct place. Commonly, specialists return to the Planner or Orchestrator agent. In our setup, we implement that by having each agent’s `run()` method return its result to the Ray caller (often the Planner waiting on `ray.get` as above). The Planner (or Judge at the end) then collates the results. If something is unsatisfactory (determined by QA/Critic feedback or Judge evaluation), the Planner can route the task again (e.g., ask the Coder to retry with Critic’s advice). This essentially forms a loop until the Judge deems it complete. By structuring these loops with Ray, we maintain **fault tolerance** – if an agent fails or produces bad output, another agent can catch it and the system re-enters a task step instead of giving up.

To give a concrete example of the routing logic: suppose the user asks for a complex piece of software. The Planner breaks it into two modules: one perhaps needing research (to find best algorithms) and the other straightforward coding. The Planner issues a research task to the Researcher and a coding task to the Coder in parallel. Once the Coder returns a draft and the Researcher returns a report, the Planner passes the research info to the Critic (to see if the code utilized the best approach) and the code to QA (to run tests). The Critic might determine that the code didn’t implement something from the research report and flag an issue. The QA might also catch a failing test. The Planner (or Judge) sees these and sends the feedback to the Coder for a second iteration. The Coder, possibly with guidance from the Critic’s notes, fixes the code and returns. QA runs again on the new code. If all tests pass and Critic is satisfied, the Judge agent then approves the final code. This kind of complex loop is made reliable by Ray’s ability to handle asynchronous task execution and waiting, as well as our structured agent protocols that allow the system to *introspect* what happened (e.g., QA’s report says “Test X failed” which the Planner can interpret as a signal to loop back).

In summary, **Ray provides the backbone** for our multi-agent orchestration: it runs agents concurrently where possible, enforces execution order where needed, and can scale the system from one machine to a cluster easily. We keep the orchestration logic separate from agent internals, which aligns with SPARC’s idea of scaffolding – the “scaffold” here is the Python code (and config) that sets up these agent tasks and their flow, rather than having agents call each other in an entangled way. This design enhances maintainability and clarity of the overall system.

## Prompt Scaffolding and Agent Interaction Protocols

A critical part of aligning with SPARC is designing **prompt templates and scaffolding** for each agent role. Rather than using plain instructions, each agent is given a structured system prompt that defines its role, objectives, and the expected format of input/output. This structured prompting ensures consistency across interactions and that each agent has a clear understanding of its duties in context. For example, the Planner’s system prompt might explicitly say it is the “Orchestrator” and list its *role-specific instructions* (like analyzing tasks for decomposition, delegating to the right agent, verifying outputs). The Researcher’s prompt will emphasize its evidence-gathering process and citation requirements. By front-loading these guidelines, we reduce ambiguity and steer the model’s behavior in the desired direction for that role.

**Multi-tiered instructions:** In complex tasks, providing all context to an agent can overflow token limits or cause confusion. SPARC-inspired prompting uses *scaffolding of context* in tiers. We will adopt a **tiered context loading** approach as described in the SPARC prompt engineering example. This means the agent prompt is divided into sections: Tier 1 (always include essential context: current objective, critical constraints, etc.), Tier 2 (additional context that can be provided on demand, like background info or previous decisions if the agent asks for them), Tier 3 (extended context only if absolutely necessary). Agents are instructed how to request more info if needed. This way, each agent starts with just the information it needs to begin, but has a mechanism to pull more in a controlled manner. This strategy maximizes **token efficiency** and prevents context overload, while still allowing access to detailed information when required (the agent explicitly asks for it, following the scaffold).

**Structured outputs:** As mentioned earlier, agents follow a template for outputs (especially those going back to the Planner or Judge). For instance, when a specialist agent completes a task, it doesn’t just return free-form text; it returns under headings like *“Task Completion Summary,” “Deliverables,” “Issues Encountered,” “Next Steps Recommendation”*, etc., along with metadata (task\_id, status). We enforce this by including an output format section in the agent’s prompt. This scaffolding allows the receiving agent (or the system) to parse and log results systematically. It also ensures no important aspect is omitted – e.g., every task return explicitly lists issues encountered and suggestions, so the Planner is always informed if something was problematic. This addresses one of the key SPARC takeaways: *“structured templates ensure consistent and complete information”* in communication.

**Cognitive patterns:** Each agent’s prompt can also include guidance on *how* to think. SPARC embeds “cognitive frameworks” like *Strategic Planning (Define→Infer→Synthesize)* or *Evidence Triangulation* into prompts. Our Planner’s prompt might instruct it to apply *Define/Infer/Synthesize* when breaking down a project (as in the documentation overhaul example). The Researcher might be told to use *Evidence Triangulation* – find multiple sources and compare. These explicit thought patterns encourage the LLM agent to follow a logical problem-solving approach, much like a checklist or methodology a human expert might use. This improves the robustness of their reasoning.

**Role-specific style:** The tone and detail of each agent’s output are tuned to its audience. For example, the Researcher’s report will be detailed and cited (intended for consumption by other agents or a human reviewer), whereas the Coder’s output is code (plus possibly inline comments or a brief summary if handing to a human). The Critic might produce a list of bullet-point critiques. The Judge might output a concise verdict or a report of why it failed and what to do. Ensuring these differences helps when we later feed outputs as inputs to other agents. It also makes logs easier to interpret. We provide examples in the prompt (one-shot or few-shot) for each agent demonstrating the expected style. For instance, the Critic’s prompt may include an example of taking a piece of code and writing a review pointing out a bug and suggesting a fix. This technique grounds the agent in its persona and task.

In summary, prompt scaffolding in our system means **meticulously engineered system prompts and interaction rules** that align with the SPARC framework’s structured approach. The agents don’t operate on ad-hoc prompts; they follow a playbook. This not only leads to better performance (by narrowing the model’s focus to the relevant context and skills) but also makes the overall system more interpretable and easier to debug, since each agent’s communications are well-structured. As a concrete outcome of this design, our agents should demonstrate improved coherence and reliability – for example, the Researcher will consistently provide thorough, source-backed research rather than a shallow answer, because its prompt and role demand that level of rigor.

## LLM Model Serving with vLLM and GPT-4.1 Fallback

To power the intelligence of each agent, we deploy a combination of **open-source Large Language Models** locally (for cost efficiency and data privacy) and allow an optional fallback to GPT-4.1 for especially difficult queries or as a safety net. The local models will be served using **vLLM**, a high-performance inference engine for LLMs that supports efficient throughput and memory utilization.

**vLLM for Local Models:** We plan to host two primary models via vLLM:

* **Phi-3.5 (Multimodal)** – This is an open small language model from Microsoft’s Phi series that can handle both text and vision inputs. It’s like a lightweight GPT-3.5 with multimodal capability. Phi-3.5 is attractive for an MVP because it’s *state-of-the-art in its class yet relatively compact* (the “mini” variant has \~3.8B parameters) and supports an extended context window (up to 128k tokens). We’ll use Phi-3.5 to give our system some multimodal ability – for example, if our Researcher agent needs to analyze an image or PDF or if we want the system to generate or parse images/charts as part of outputs. vLLM can load the Phi-3.5-vision model and serve it on an HTTP endpoint. The model card notes it underwent fine-tuning and preference optimization to follow instructions accurately and safely, which suits our needs for an aligned agent.

* **“Gemini” class model (Open-Source)** – By “Gemini OSS”, we refer to an open-source model analogous to Google’s Gemini (which is a next-gen multimodal model) but available for self-hosting. Since Google’s actual Gemini is proprietary, we will integrate an open model that approaches GPT-4-level performance on text tasks. This could be something like **Llama 2 70B**, **Mistral 13B (if released)**, or a composite model ensemble named “Gemini” by the community. Notably, the Medium article’s benchmark table compared a “Gemini-1.5-Flash 8B” with other models – presumably a smaller variant. In our case, we might use an open large model for pure text generation, code understanding, etc., leaving multimodal tasks to Phi-3.5. This model would be served via vLLM as well, possibly on a different endpoint or with a model-switching mechanism. The goal is to cover complex reasoning and coding abilities beyond what a 3.8B model can do, by using a larger foundation model (for instance, Llama-2 70B has demonstrated strong performance close to GPT-3.5). We ensure the model is truly open or permissible for local deployment.

vLLM provides an **OpenAI-compatible REST API** for served models, meaning our agent code can query the local models using the same calls as it would OpenAI’s API (completions/chat completions). This simplifies integration: each agent can be implemented to call an LLM service without hardcoding a specific model. At runtime, we can direct those calls either to our vLLM server (for Phi-3.5 or Gemini model) or to the real OpenAI API for GPT-4.1 as needed. vLLM’s optimized backend uses techniques like PagedAttention to achieve high throughput, so it can handle multiple concurrent agent requests and long contexts efficiently on our hardware. This is crucial since our Planner might feed a large context (specification, multiple agent outputs, etc.) into the Judge or Critic agent. With vLLM, using the Phi-3.5 model’s 128k token window is feasible, and the system can stream results for responsiveness.

**GPT-4.1 Fallback Proxy:** While we strive to use local models, it’s prudent to have access to the powerful GPT-4.1 (assuming by 2025 this refers to an updated GPT-4 model with possibly 4.1 iteration) for cases where the open models fall short. We integrate a *fallback mechanism* in the agent orchestration: if a query or subtask is too complex or if the local model’s output confidence is low, the system can route the prompt to GPT-4.1 via API. To do this cleanly, we run a small **proxy service** that mimics the OpenAI API (so it exposes the same `/v1/chat/completions` endpoint, for example) but internally forwards requests to the real OpenAI service (with an API key). This allows our agents to switch between local vLLM and OpenAI by just changing the base URL or by a configuration flag.

For example, we might configure our system with thresholds: if the Critic agent finds that an answer is not satisfactory even after a couple of iterations with local models, the Planner could escalate that subtask to the GPT-4.1 proxy. Or the user might specifically tag a task as requiring high reliability, in which case we use GPT-4.1 from the start (this could be a CLI option like `--use-gpt4`). Since GPT-4.1 would incur cost and need internet, we don’t want to use it for everything, but it’s available as a **back-stop for quality**. The proxy ensures that using GPT-4.1 is as seamless as using a local model – the agents don’t need to know, they just call an endpoint.

Additionally, using a proxy means we could later swap it for another OpenAI-compatible service, such as an open router or another vendor API (Anthropic’s Claude or others) if needed, by conforming to the same API spec. SPARC CLI already had support for multiple providers (OpenAI, Anthropic, OpenRouter), so we emulate that flexibility.

**Model selection per agent:** We will likely assign different models to different agent roles, to optimize performance. For instance:

* The Coder and Critic might use the **Gemini/large model** because coding and deep reasoning benefit from a more powerful model (and require a bigger context for code).
* The Researcher could use **Phi-3.5** for reading documents or handling image data, since it can do OCR or image understanding and has a large context for long documents.
* The Planner/Judge might use the larger model for best overall reasoning (or might try Phi-3.5 first and escalate to GPT-4.1 for final judging if unsure).
* QA agent might not need a huge model; if it’s running actual code tests, minimal language reasoning is needed aside from interpreting results. But if QA is used for logical validation, the larger model might help to accurately analyze complex outputs.

We also ensure that all these models are running **locally** (except GPT-4.1 when called). Running a 70B model and an 3.8B model concurrently might require a strong GPU setup, but since this is MVP, we assume the host has either the necessary GPUs or we configure to use quantized models for CPU if needed. vLLM can manage multiple models by launching separate instances (each listening on a different port or with different model identifiers). Our Docker Compose will have services like `vllm-gemini` and `vllm-phi` for example, each running the vLLM engine with the respective model weights.

To summarize, the system’s LLM backbone is **local-first**:

* We use **Phi-3.5 (open multimodal)** and a **Gemini-class open model** to handle the bulk of tasks without external calls.
* We serve them efficiently through **vLLM**, benefiting from high throughput and large context windows.
* We incorporate a **GPT-4.1 fallback** via an OpenAI-compatible proxy for tasks requiring extra muscle or as a safety valve for accuracy.
  This approach balances cost, capability, and privacy, and provides a flexible foundation where models can be swapped/upgraded as open-source offerings improve.

## Memory and Observability: Langfuse, ClickHouse, Redis, and Vector DB

A hallmark of our system is that it maintains **memory** of interactions and improvements over time. We achieve this through two levels of memory:

1. **Event/trace memory** for observability and analysis (long-term logs of what happened), and
2. **Semantic memory** for recalling knowledge across sessions (persisted vector embeddings of content).

### Observability with Langfuse (Trace Memory)

We integrate **Langfuse**, an open-source observability platform for LLM applications, to log and visualize all agent activities. Every prompt sent to an agent, every model response, every tool usage, and custom metrics (like tokens used, latency) will be sent to Langfuse. This gives us a rich **trace** of each task execution, which is invaluable for debugging, monitoring performance, and informing the self-improvement mechanism.

Langfuse in self-hosted mode relies on a couple of components:

* **ClickHouse database**: a fast open-source OLAP database used to store Langfuse traces (events, observations, scores) with high write throughput. We will run a ClickHouse instance to accumulate our logs. ClickHouse is very efficient for time-series and analytical queries, so we can query things like “What was the average token count used by the Researcher agent?” or “Show all tasks where the Critic suggested changes” in near real-time.
* **Redis**: used by Langfuse as a caching layer and job queue to ingest events quickly. When an agent sends a log event (via Langfuse SDK or API), it first goes into Redis, and the Langfuse worker takes from Redis to insert into ClickHouse asynchronously. This decoupling ensures minimal overhead on our running agents – they can fire off logging events and continue with minimal delay. We run a Redis (or its alternative Valyr) instance configured with no-eviction policy (so queue entries aren’t lost).

Langfuse itself consists of an **application server** (which provides a web UI and API to query traces) and a **worker** (that processes the events from Redis into ClickHouse). Our deployment will include containers for `langfuse-web` and `langfuse-worker`, connected to the Redis and ClickHouse containers. We’ll configure Langfuse via environment variables to point to our local ClickHouse (`CLICKHOUSE_URL`) and Redis (`REDIS_CONNECTION_STRING`) services. All interactions will be logged through Langfuse’s API – we’ll use their Python SDK in our code, so each agent call does something like `langfuse.trace(event_name="prompt_sent", data={...})` with details.

With this in place, we have **long-term memory of all sessions**. Even if the system restarts, previous tasks’ logs remain in ClickHouse for analysis. We can replay what sequence of prompts led to a failure or success, which is crucial for the self-improvement loop. Langfuse’s UI will allow us to see each agent’s thought process (the prompts and responses) and timeline. Moreover, we can define **custom metrics** or tags. For example, when the Judge agent finishes a task, we might log a “score” indicating success or the number of iterations needed. These can be aggregated later to see trends.

For privacy, since this is self-hosted, all this data stays local. If there are multiple developers or users, they can also use Langfuse to collaborate and review agent behavior in a familiar interface (it’s akin to Application Performance Monitoring but for AI reasoning traces).

### Session Memory (Short-Term Context)

While Langfuse covers logging, **session memory** is about the working memory that agents share within a single task or conversation. In our design, the Planner agent will maintain a *context object* for each task, which contains things like the overall objective, key facts discovered, interim conclusions, etc. Agents can read/write to this context via a **Memory Tool** (similar to how SPARC CLI had a memory tool). For example, after the Researcher agent returns findings, the Planner might store those findings in the context memory so that the Coder or Critic can access them later without reprompting the Researcher.

We can implement session memory simply as an in-memory Python dictionary passed around or something more robust like a **Redis-based store** for quick read/write access. We already have Redis running for Langfuse; we could use a separate Redis database or namespace for storing session data (keyed by task\_id or session\_id). This way, if the process dies mid-task or we scale to multiple machines, any agent can retrieve the session data by querying Redis. This also fits with the concept of using Redis for ephemeral data due to its speed. It’s not strictly necessary to persist session memory long-term (once task completes, the important bits should get distilled into long-term memory if needed), but we could keep it around for some time for follow-up queries.

We will provide the agents with access to this session memory through a tool or API. For example, an agent could call `memory_tool.save("research_summary", summary_text)` and later another agent can call `memory_tool.load("research_summary")` to get it. The Memory Tool logic will interface with the Redis store for these operations. This allows decoupling: an agent doesn’t need the entire conversation every time – it can fetch specific pieces by key (some keys might be auto-generated by the system, like each subtask’s result gets a key).

### Persistent Semantic Memory (Vector Database)

Beyond immediate sessions, the system will accumulate knowledge that can be reused in future tasks. This includes previous code it wrote, solutions to problems, domain knowledge, etc. To enable the system to *recall* relevant information from past tasks, we incorporate a **Vector Database** (either **Chroma** or **Qdrant**) as a persistent semantic memory store.

How it works:

* When an important piece of information is generated or encountered, we create an embedding of it and store it in the vector DB with metadata. For example, after completing a task, we can embed the problem description and the final solution summary, storing that so similar future problems can retrieve this solution. Or we embed code snippets it wrote, so next time if a similar function is needed, the system can recall it rather than coding from scratch.
* For knowledge ingestion, if we have project documentation or large reference texts, we can pre-embed chunks of that into the vector DB so the Researcher agent can quickly query it via similarity search rather than doing a web search each time.

We can choose **ChromaDB** for simplicity (it can run in-memory or with persistence to disk) or **Qdrant** (runs as a standalone vector DB server with good performance and filtering capabilities). Qdrant might be more production-ready for scale, so let’s assume we use Qdrant in a container. The integration will be done via an embedding model: likely we use the same LLM (or a smaller embedding model) to produce embeddings. OpenAI’s `text-embedding-ada-002` is great but to keep it local, we could use something like **InstructorXL** or **SentenceTransformers** model for embeddings.

The **memory retrieval** will be exposed to agents via a *Knowledge Base Tool*. For instance, the Planner or Researcher can query the vector DB with a question or keywords. The system will then do a similarity search and return the top relevant vectors (with their original text). The Researcher agent could incorporate this into its answer. If the vector DB contains code history, the Coder agent could query it (e.g., “did we implement something similar before?”) and get back a snippet to reuse, accelerating development.

SPARC 2.0 emphasizes a vector store as a key element: *“At the heart of SPARC lies its vector store, a specialized database that transforms code and text into abstract patterns… enabling the system to locate similar code snippets despite differences in variable names or styles”*. We emulate exactly this. By storing abstract representations of prior outputs, our system gains a **long-term memory** that is not limited by context window. It can make our agents more efficient and contextually aware over time. For instance, if the system has solved a task in a certain domain before, the Planner might retrieve that plan from memory and adapt it instead of starting from scratch.

In terms of deployment, if using Qdrant, our Docker Compose/Helm will include a `qdrant` service. If using Chroma, we might just run it as part of the main app (embedded) for ease, but to keep separation, using Qdrant is fine. We will volume-mount storage for Qdrant so that vectors persist across restarts.

**Redis vs Vector DB**: It’s worth clarifying their roles – Redis is for short-term exact key-value memory and queueing (non-semantic), whereas the Vector DB is for semantic similarity search on large content. They complement each other. We avoid using Redis as a vector store (though it has modules for that) to keep architecture specialized: Qdrant/Chroma excels at vector ops and indexing.

### Putting it Together:

When an agent (like Researcher) finds a crucial piece of info, the system logs it to Langfuse (trace), stores it in session memory (Redis) for immediate use, and possibly indexes it in vector DB for future recall. Later, if a similar task arises, the Planner or Researcher can do a semantic lookup. Over time, this builds a knowledge base the system can learn from. This design also aids the **auto-updating capability**: by analyzing past traces and using embeddings of those traces, the system’s Critic agent might identify patterns (like “whenever we do frontend tasks, the code agent struggles with CSS – perhaps we need a new agent for UI design”). That kind of insight can be gleaned by vector-searching past issues and summarizing them.

To close the loop on observability: we will define some dashboards or at least use Langfuse’s UI to monitor metrics – such as token usage per agent, error rates, turnaround time per task, etc. This operational data is stored in ClickHouse and can be queried anytime to see how the system is performing or where bottlenecks are.

In conclusion, **Langfuse + ClickHouse + Redis** gives us a solid observability and debugging backbone (every decision is recorded, enabling improvement), while **Chroma/Qdrant** provides the system with a growing brain of its own experiences and knowledge. These elements fulfill the SPARC idea of an AI system with *memory* and *self-awareness* of its work, rather than a stateless prompt executor.

## Deployment Architecture (Docker Compose & K3s/Helm)

We provide a complete deployment package that supports both a simple Docker Compose setup (for local development or single-machine use) and a Kubernetes-based setup (using K3s and Helm for a lightweight cluster or full K8s for production). This ensures the system can be run **locally first** and then scaled or managed in a cluster if needed.

### Docker Compose Setup

In the Docker Compose configuration, we define multiple services corresponding to the components described:

* **ray-head**: A container running the Ray head node and our main application (the orchestrator and agents code). This container will likely use a custom image (built from our repository code) that installs Python dependencies (Ray, the LLM libraries, Langfuse SDK, etc.) and on startup it launches a Ray head. It may also launch the CLI interface container (or we can run CLI manually inside it). The environment for this container will include things like `PYTHONPATH=/app/src`, and possibly GPU configurations (if using GPUs for models).

* **ray-worker** (optional): If we want to simulate a multi-node environment or just separate processes, we could include one or more Ray worker containers that join the head. For an MVP on one machine, the head can also spawn workers internally. Compose allows deploying N instances of a service if scaling horizontally.

* **vllm-phi**: A container running vLLM serving the Phi-3.5 model. This would use an image (maybe `ghcr.io/vllm/vllm:latest` or a custom image with the model weights baked in). It will expose a port (say 8000) for the OpenAI API compatible interface. We will mount the model weights (or have the container download from HuggingFace on first run). Environment might specify model name (`MODEL_NAME=microsoft/Phi-3.5-vision-instruct`) and some performance tuning params.

* **vllm-gemini**: Another vLLM container for the larger model (if using one). This might require more memory/GPUs. It exposes another port (say 8001). In our config we’ll label these so that the agent code knows which base URL to call for which agent (for example, Coder agent calls the gemini endpoint by default).

* **openai-proxy**: A lightweight container (perhaps just using `node:alpine` or a tiny Flask app) that forwards requests to OpenAI. Alternatively, we might omit this and simply allow direct calls from the Ray container to OpenAI API if given an API key. But having a proxy allows centralizing rate-limit handling and logging. We’ll include it for completeness. It might listen on port 8002 and expect the same payload as OpenAI’s API. (This service would not be used unless configured, and the user will have to supply an API key via env var).

* **langfuse-web**: Container for Langfuse’s web UI and API. Langfuse provides a Docker image for this (the open-source version). We configure it with env vars for connecting to ClickHouse and Redis (e.g., `CLICKHOUSE_URL=http://clickhouse:8123` and `REDIS_CONNECTION_STRING=redis://redis:6379` etc.). We also might set an admin token if needed for the UI. The web service by default might run on port 3000 (we can map it to a host port so we can open the Langfuse dashboard in browser).

* **langfuse-worker**: Container for Langfuse’s background worker. It will have similar env configuration (ClickHouse, Redis) and it basically just processes the queue. In a small setup, we might combine web and worker in one container, but Langfuse typically separates them for reliability.

* **clickhouse**: A container running ClickHouse DB (using the official image). We’ll configure persistence (volume for data) so logs aren’t lost on restart. The default port 8123 (HTTP) and 9000 (native) can be exposed internally. Langfuse will use HTTP interface for inserts/queries. We might not expose these ports to host for security (not needed externally).

* **redis**: A container for Redis. We’ll enforce `maxmemory_policy noeviction` in its config to align with Langfuse’s requirement (to never drop queue items). This can be done via a config file or command option. We’ll likely just use it for Langfuse and our memory; a single instance is fine for MVP. Port 6379 internal only.

* **qdrant** (or **chroma**): If Qdrant, a container from the official image which exposes port 6333 (for API). We’ll give it a volume for persistence. If Chroma, possibly running as part of Ray container (Chroma can be used in-memory or with an embedded disk store inside the Python process, which might be simpler for MVP). Let’s assume Qdrant for clarity. The vector DB service will allow gRPC or REST; our app likely uses REST or the Qdrant Python client to upsert/query.

* **sparc-cli** (optional): We could have a separate service for an interactive CLI, but more likely, the CLI is just a command we run on the ray-head container. Alternatively, we can run a one-off container that connects to Ray’s head for CLI operations. For example, `docker compose run sparc-cli create-task ...` which execs into the Ray container context. However, to simplify, the user might just `docker exec -it ray-head /bin/bash` and run the CLI commands. Nonetheless, we will provide the CLI tool within the same image.

All these services and their relationships (networks, dependencies) will be defined in `docker-compose.yml`. We ensure that:

* Ray-head starts before workers, and possibly before CLI usage.
* Langfuse-web waits for ClickHouse and Redis (we can use Compose’s depends\_on but might also need a retry logic inside container because depends\_on doesn’t guarantee ready state).
* The Ray container can reach vLLM containers (all are on the same Docker network by default).
* Environment variables for sensitive things (OpenAI API key, etc.) are passed appropriately (likely via a `.env` file or instruct user to export before `docker compose up`).

With Compose, a user could do `docker compose up -d` and get the whole stack running locally. They’d then attach to the CLI or use an API to send tasks.

### K3s + Helm Deployment

For a more production-like or scalable setup, we include **Helm charts** for Kubernetes. We will have a chart (or a set of sub-charts) that mirror the services above:

* A Deployment for the Ray head (with a Service for it if needed, maybe not external).
* Possibly a HorizontalPodAutoscaler for Ray workers or a separate deployment you can scale.
* Deployments for vLLM instances, each likely with a LoadBalancer or ClusterIP service (so that the Ray pod can call the vLLM pods via a stable DNS name).
* Deployment/StatefulSet for ClickHouse (StatefulSet is better for DB to ensure stable storage and hostnames).
* Deployment for Redis (or use a bitnami helm dependency).
* Deployment for Qdrant (StatefulSet if we want persistence).
* Deployments for Langfuse web and worker.
* ConfigMap/Secrets for configuration (e.g., config file for Redis, env for API keys, etc.)
* PersistentVolumeClaims for ClickHouse data, and maybe for Qdrant if needed.
* Possibly an Ingress for the Langfuse web UI (so user can access the dashboard through an HTTP route).
* Optionally an Ingress/Service for the OpenAI proxy if user wants to call the system’s models from outside (though not needed for internal workings).

Our Helm templates will allow toggling components (for instance, in a dev environment you may not deploy GPT-4 proxy or may choose Chroma vs Qdrant via a value flag). We will make use of existing charts if possible: for example, Langfuse might provide a Helm chart (they have k8s guides with a Valheim or something), but to keep it contained, we can incorporate it.

**K3s** is just a flavor of Kubernetes, so our chart should work on any K8s (but using lightweight storage class for local PV etc., K3s makes that easy). We’ll document how to install: basically, `helm install sparc-mvp ./helm` after adjusting values (like setting `openaiApiKey` in a secret, etc.).

Important config details:

* We ensure that each microservice’s resource requirements are set (the large model might need a GPU node, so we might put a nodeSelector or toleration for GPU nodes for vLLM-gemini deployment).
* The vector DB and DBs should ideally have persistent volumes; we’ll use local-path storage in K3s or let user configure NFS/EBS in cloud.
* We externalize as values any image names/tag, so one can update the model images or our app image easily.
* Networking: Probably all internal comms, except we might expose Langfuse UI and maybe an API endpoint for external use (perhaps not needed since CLI is the interface).
* If needed, the CLI could also be run as a Kubernetes Job or a Pod (for example, `kubectl exec` into the Ray head to run CLI commands). We could provide a simple script to do that.

The **auto-update** mechanism (to be discussed next) might involve the system updating its config maps or Helm releases. Helm is static by itself, but the agents could potentially rewrite the manifest files (YAMLs) in our repository and prompt the user to re-deploy. For a running cluster, a more advanced approach could be to have the system apply changes via Kubernetes API (which is a bit out of scope for MVP, but possible through the Python client). However, initially we might handle auto-updates by having the system update the local files and notify that a helm upgrade can be run.

### CLI Interface and Usage

We have built a CLI tool (possibly just part of the main Python package, invoked via console\_scripts entrypoint named `sparc`) to allow users to interact with the system. The CLI provides commands to **create tasks**, monitor progress, and manage the self-improvement process:

Key CLI commands:

* `sparc new "<task_description>" [--interactive] [--use-gpt4]` – This creates a new task in the system. The description is given to the Planner agent to start the SPARC workflow. If `--interactive` is set, the CLI will keep the session open, streaming the agents’ outputs as they work (the CLI can tail the Langfuse logs or get callbacks from the Planner). If not interactive, it will return a Task ID and immediately exit, while the task runs in background. `--use-gpt4` could force the Planner to allow GPT-4 fallback if it sees fit for that task.
* `sparc status <task_id>` – Query the current status of a task. The CLI will fetch from our system whether the task is still running, and perhaps summary of what’s done. It might poll an internal endpoint or simply check if the Judge produced an outcome in the database. For more detail, we might output the last few log messages or agent interactions.
* `sparc review <task_id>` – Once a task is complete, this can display the final output and possibly a structured report of the process (like which agents were involved, how many iterations, etc.). Because we have Langfuse, we might also just provide a link to the Langfuse UI filtered by that task run (Langfuse allows tagging traces with e.g. `task_id`, so user can see a timeline).
* `sparc list` – List active or recent tasks with their IDs and status.
* `sparc kill <task_id>` – Cancel a running task (this would signal the Planner to stop, and Ray to cancel tasks if possible).
* `sparc agent-console <agent_name>` – (Optional) open an interactive REPL with a specific agent. This is akin to SPARC CLI’s interactive chat mode. For instance, you can talk directly to the Researcher agent via CLI for ad-hoc queries. This is not core to MVP but a nice debugging feature.

The CLI is implemented with a user-friendly library (could be Python’s Click or Typer) and will have colored output, etc., similar to SPARC CLI which had rich formatting. We preserve the “human-in-the-loop” capability: if an agent is waiting for approval or input (perhaps our Planner can ask the user a question via a special prompt), the CLI will detect that and present it to the human, allowing them to respond. For example, if the Judge isn’t sure about deploying without human review (as suggested in Pallavi’s example with a Publisher agent requiring human double-check), the CLI can prompt “Agent requests human input: Do you approve deployment? (y/n)”. The user input would then be fed back into the system (likely to the Judge or Planner).

Under the hood, the CLI communicates with the running system. Since our system is running as processes in Ray, we need a mechanism to query status and send input. Several approaches:

* Use Ray’s built-in Job API or Actors: We can register the Planner (or a Controller actor) in a named registry. The CLI, when run (which is just another Python process), can connect to Ray (if on same network) and invoke methods on that actor. E.g., `ray.get_actor("Controller").create_task.remote(task_desc)`. This would require the CLI to have access to Ray’s address. In Docker Compose, we could allow the CLI container to join the ray network and use the head’s address.
* Expose a minimal REST API from the main app: For instance, run a FastAPI server on the Ray head container that exposes endpoints for create task, get status, etc. The CLI then simply HTTP POSTs to create a task. This might be simpler and more decoupled. We might do this for ease of integration (less coupling with Ray’s internals).
* Or even use Redis/pub-sub: CLI could publish a message to a “new\_task” channel and the Planner subscribes. But an HTTP API is straightforward.

Given time, implementing a small API endpoint `/tasks` with GET/POST might be easiest. We can use Python’s FastAPI inside the main container (Ray won’t conflict; it can run alongside or as a task). This also means even without CLI, one could hit the API to use the system (could integrate with another app). We’ll include this as a note.

So, the user story with CLI: user writes `sparc new "Build a web scraper that collects prices from e-commerce site"`. The system (Planner) will start working; CLI maybe streams high-level updates. Eventually, CLI prints the final result (maybe the code or summary) and indicates task done. If the user instead runs without `--interactive`, they can detach and later run `sparc review` to get results.

### Deployment Considerations

Both deployment methods ensure all parts of the system can talk to each other and are secure locally. For Docker Compose, everything is on a private network by default. For K3s, we restrict services to cluster-internal except the ones needing exposure (Langfuse UI, possibly the API or UI for tasks if we add one). We include default credentials or tokens for services as needed (for instance, securing Langfuse could be done via basic auth or a generated token).

We also ensure **auto-restart** for components on failure (Compose can use `restart: on-failure`, K8s uses restartPolicy). This way, the system is resilient: e.g., if vLLM crashes due to OOM, it restarts and the Planner can retry that query.

## Auto-Updating and Self-Improvement Mechanism

One of the most ambitious features is enabling the system to **update its own configuration, manifests, and workflows** using its AI capabilities – essentially *self-evolution*. This aligns with SPARC’s vision of *continuous self-improvement through feedback loops* and the concept of agents that can refine their own prompts and code over time. We implement this in our MVP in a careful, controlled way.

**Manifests and DAG Definitions:** In our architecture, a “manifest” could be a configuration file (YAML/JSON) that defines agents, their prompts, and the task routing DAG. For example, we might have a `agents.yaml` listing each agent role, which model it uses, and its system prompt template. And a `workflow.yaml` that defines sequences like: “On task creation: Planner -> (Researcher & Coder in parallel) -> Critic -> Coder -> QA -> Judge”, etc. Initially, these are designed by us.

The **auto-update** means the system’s agents themselves can modify these files to improve performance. We facilitate this by treating the manifest and workflow definitions as just another piece of text that the agents (particularly the Coder and Critic) can edit.

Here is how an auto-update cycle might work:

1. **Trigger:** After each task (or after a certain number of tasks or on a schedule), the system triggers a special meta-task: “Review and improve system itself.” This could also be triggered manually via CLI (`sparc self-improve`).
2. **Data gathering:** The system uses its logs and memory to identify pain points. For instance, the Critic agent could be tasked to analyze the Langfuse traces of recent tasks to find inefficiencies or failures. It might observe, for example, that *“the Coder agent often asks the Researcher for API details, maybe we should have an API documentation knowledge base integrated.”* Or it finds *“the Planner frequently assigns a task to the wrong agent first and then has to reassign – maybe update routing logic.”* These observations can be gleaned by looking at where extra iterations occurred or where a human had to intervene.
3. **Proposal:** The Critic (or a dedicated “Architect” agent) formulates a proposed change to the manifest. This might be written in natural language first, like “Add a new agent role for UI/Design” or “Change the prompt for Researcher to always include a web search tool usage example”. It could also directly propose a diff/patch if it’s confident (e.g., “In agents.yaml, change the Critic’s instructions to emphasize security checks.”).
4. **Validation of Proposal:** The system should verify the proposed changes. The Judge agent (or even the Planner in a special mode) will evaluate if the proposal makes sense and won’t break things. This is analogous to a human reviewing a pull request. The Judge might simulate a scenario with the new config mentally or just check that the rationale addresses the issue. If we had a test suite for the AI system itself, it could run it (for example, run a known task both with old and new config in a sandbox and compare results, though that’s advanced).
5. **Implementation:** If the proposal is accepted, the **Coder agent** will apply it. Since everything is text-based config or code, the Coder can open the manifest file (we provide file system access in a restricted manner) and edit the necessary sections. For safety, we could require it to produce a unified diff as output, and the system applies that diff.
6. **Deployment of Change:** Once the manifests are updated, the system can either live-reload them or schedule a restart. For some changes (like prompt tweaks), we can apply them on the fly to agent instances. For bigger changes (like adding a new agent class), it might require restarting the orchestration layer to instantiate that agent. In Kubernetes, the system could theoretically patch its ConfigMap and initiate a rollout of the deployment. In Docker, it might simply log “please restart to apply changes.” For MVP, we might apply easy changes live and defer complex ones until a manual restart.

Throughout this process, human oversight is advisable at least initially. We can configure the Judge agent to always ask for human approval before actually committing the change. The CLI can show the diff of what’s about to change and prompt “Apply this update to system? \[y/N]”. This ensures we don’t get runaway self-modification without supervision (an AI safety measure).

**Example auto-update scenario:** Suppose the system notices that the `Critic` agent often points out style issues in code that the `Coder` could have caught (like simple linting). The Critic proposes: “Let’s integrate a linter tool for the Coder.” It drafts an update: modify the Coder’s prompt to include a step “Check code against PEP8 and fix style issues” and perhaps add a new tool in the Coder’s toolbox (maybe a call to a `pylint` command via the Shell tool). The Judge reviews and finds this reasonable, and the user approves. The Coder agent then edits `agents.yaml` to add that instruction in the Coder’s prompt and `tools.yaml` to register the pylint tool. The system then uses these updated files for the next tasks. Over time, these incremental improvements make the system more autonomous and efficient.

Another scenario: The system might realize it needs a new agent role entirely, e.g., a “Security Auditor” agent to specialize in checking for vulnerabilities. It could then literally append a new section in the agents config, define that agent’s prompt (likely using its own knowledge or even asking GPT-4.1 for help writing a good prompt for that agent), and update the workflow to insert the Security Auditor at the appropriate point (perhaps after Coder and before final Judge). This is complex but within the capability of an LLM given enough context. We could facilitate this by providing meta-templates or guidelines in the system prompt for self-improvement, like “Here is the schema of the config file, here’s how to add an agent.”

This **self-modification ability** turns our system into a sort of *autonomous evolving agent pipeline*. It resonates with emerging research (like Agent-as-a-Judge for automated feedback and frameworks where agents breed new agents). Our approach ensures changes are persisted (since the manifests are files on disk or configmaps). We might even version-control these config files in a git repo. Possibly, the system can commit to a local git and annotate what it changed. This provides an audit trail of its evolution, and we can roll back if it went astray.

Of course, there’s a risk associated with self-rewrites. We mitigate it by:

* Only allowing certain scopes of changes (we can have the Critic agent’s proposal be constrained to the YAML schema known, and not allow arbitrary code execution changes unless reviewed).
* Human confirmation step for potentially dangerous changes.
* Testing changes on a sample task if possible before full acceptance.

SPARC was designed with the idea of *conscious improvement* in mind (the GitHub readme even jokes about “quantum” consciousness and integrated feedback loops). While we’re not implementing anything quantum, our feedback loop is the Critic/Judge analyzing the system’s own performance and making adjustments. In essence, the system treats its own configuration as another codebase to refactor – using the same multi-agent approach (Planner identifies improvement areas, Coder implements changes, QA tests the system, etc.). It’s a bit meta, but achievable.

In practice for MVP, we will demonstrate a simple case of auto-update: perhaps the system will auto-tune some parameter in prompts (like increasing the context it includes, or adjusting a temperature setting for an agent) based on observed outcomes. We’ll show that the agents can rewrite a config file and the new setting takes effect.

## Conclusion

In this comprehensive MVP, we have constructed a **local-first multi-agent LLM system** that mirrors the SPARC framework’s structure and philosophy. The system comprises specialized agents (Planner, Researcher, Coder, QA, Critic, Judge) working in a coordinated pipeline to tackle complex tasks in stages. We orchestrate their interactions using Ray, enabling parallelism and robust routing of sub-tasks. Each agent is driven by carefully scaffolded prompts and has access to tools and memory, ensuring it performs its role with focus and context. We utilize open-source models (like Phi-3.5 and a Gemini-class model) served via vLLM for fast local inference, and provide a bridge to GPT-4.1 for additional firepower when needed. To support continuous improvement and transparency, we integrated Langfuse for observability – storing traces in ClickHouse and leveraging Redis for quick logging throughput – and a vector database for long-term semantic memory of the system’s knowledge. The entire architecture is delivered with reproducible deployment configurations: Docker Compose for one-command setup and Helm charts for Kubernetes to scale out or manage in cloud environments.

Crucially, our MVP includes an **auto-update mechanism** whereby the system can learn from its mistakes. By using its Critic and Coder agents to modify its own configuration, it embodies a self-reflective loop. Over time, this can lead to a system that not only solves user tasks but also optimizes itself – edging closer to an autonomous AI development environment that **“continuously self-improves through integrated feedback”**. All of this is done with the user in control (via the CLI and optional human-in-loop checkpoints) to ensure that automation remains aligned with user goals and safety.

In summary, this MVP lays a foundation for an AI-powered development assistant that is modular, transparent, and evolvable. It realizes the SPARC vision of an “AI symphony” – multiple specialized models orchestrated like instruments, each playing its part on cue toward a successful outcome. The system is ready to run locally, fostering privacy and ownership of one’s AI processes, but it’s equally ready to grow – by scaling on Kubernetes, by incorporating new models, or by adapting its own strategies. By open-sourcing this along with SPARC, we hope to contribute a powerful tool for AI-assisted engineering that developers can experiment with, deploy, and even have it **build and rebuild itself** into ever better versions.

**Sources:**

* R. Cohen, *“SPARC 2.0 Code Agent + MCP Server”* – LinkedIn article describing SPARC’s approach to code automation and vector memory.
* V. ResearchX, *“Structured AI Team with SPARC”* – Reddit post on multi-agent prompt engineering with SPARC, including role templates and boomerang pattern.
* D. Patten, *“From Prompt to Product: Agentic Engineering”* – Medium post outlining SPARC phases and agent roles in software development.
* P. Sinha, *“LLM-Based Multi-Agent Architecture”* – Medium post explaining multi-agent benefits (specialization, fault tolerance, etc.) and a hierarchical orchestration example.
* Adilmaqsood, *“Agent-as-a-Judge: AI Evaluating AI”* – Medium post on using AI agents to judge and improve other agents, supporting our Judge/critic design.
* MotleyCrew\.ai, *“Using Ray for Agentic Workflows”* – discussing Ray’s DAG-based orchestration for multi-agent systems.
* Microsoft, *Phi-3.5 Model Card* – details on the Phi-3.5 multimodal model (open, 128k context) we integrate.
* Langfuse Documentation – notes on using ClickHouse for trace storage and Redis for event caching in self-hosted observability.
