services:
  # Redis - Fast in-memory cache and message queue
  redis:
    image: redis:7-alpine
    container_name: vllm-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - vllm-network

  # ClickHouse - Long-term trace storage
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: vllm-clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./configs/clickhouse:/etc/clickhouse-server/config.d
    environment:
      CLICKHOUSE_DB: langfuse
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-langfuse123}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - vllm-network

  # Qdrant - Vector database for semantic memory
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: vllm-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - vllm-network

  # Langfuse Database (PostgreSQL)
  langfuse-db:
    image: postgres:15
    container_name: vllm-langfuse-db
    environment:
      POSTGRES_DB: langfuse
      POSTGRES_USER: langfuse
      POSTGRES_PASSWORD: ${LANGFUSE_DB_PASSWORD:-langfuse123}
    volumes:
      - langfuse_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langfuse -d langfuse"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - vllm-network

  # Langfuse - Observability and tracing
  langfuse-web:
    image: langfuse/langfuse:2
    container_name: vllm-langfuse-web
    depends_on:
      langfuse-db:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    ports:
      - "3000:3000"
    environment:
      DATABASE_URL: postgresql://langfuse:${LANGFUSE_DB_PASSWORD:-langfuse123}@langfuse-db:5432/langfuse
      NEXTAUTH_SECRET: ${LANGFUSE_SECRET:-your-secret-key}
      NEXTAUTH_URL: ${LANGFUSE_URL:-http://localhost:3000}
      SALT: ${LANGFUSE_SALT:-your-salt}
      ENCRYPTION_KEY: ${LANGFUSE_ENCRYPTION_KEY:-your-encryption-key}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: true
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-langfuse123}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/public/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - vllm-network

  # Langfuse Worker - Background job processing
  langfuse-worker:
    image: langfuse/langfuse:2
    container_name: vllm-langfuse-worker
    depends_on:
      langfuse-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://langfuse:${LANGFUSE_DB_PASSWORD:-langfuse123}@langfuse-db:5432/langfuse
      REDIS_URL: redis://redis:6379
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-langfuse123}
    command: ["node", "dist/src/worker.js"]
    networks:
      - vllm-network

  # vLLM - Local model serving (Phi-3.5)
  vllm-phi:
    image: vllm/vllm-openai:latest
    container_name: vllm-phi-server
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - vllm_cache:/root/.cache/huggingface
    environment:
      VLLM_HOST: 0.0.0.0
      VLLM_PORT: 8000
      VLLM_MODEL: microsoft/Phi-3.5-mini-instruct
      VLLM_TENSOR_PARALLEL_SIZE: 1
      VLLM_GPU_MEMORY_UTILIZATION: 0.8
      VLLM_MAX_MODEL_LEN: 32768
      VLLM_TRUST_REMOTE_CODE: true
    command: >
      --model microsoft/Phi-3.5-mini-instruct
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.8
      --max-model-len 32768
      --trust-remote-code
      --served-model-name phi-3.5-mini
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s
    networks:
      - vllm-network

  # vLLM - Large model serving (configurable)
  vllm-large:
    image: vllm/vllm-openai:latest
    container_name: vllm-large-server
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
      - vllm_cache:/root/.cache/huggingface
    environment:
      VLLM_HOST: 0.0.0.0
      VLLM_PORT: 8001
      VLLM_MODEL: ${LARGE_MODEL:-microsoft/DialoGPT-large}
      VLLM_TENSOR_PARALLEL_SIZE: ${LARGE_MODEL_TP:-2}
      VLLM_GPU_MEMORY_UTILIZATION: 0.7
      VLLM_MAX_MODEL_LEN: 8192
      VLLM_TRUST_REMOTE_CODE: true
    command: >
      --model ${LARGE_MODEL:-microsoft/DialoGPT-large}
      --host 0.0.0.0
      --port 8001
      --tensor-parallel-size ${LARGE_MODEL_TP:-2}
      --gpu-memory-utilization 0.7
      --max-model-len 8192
      --trust-remote-code
      --served-model-name large-model
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${LARGE_MODEL_GPU_COUNT:-2}
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s
    networks:
      - vllm-network
    profiles:
      - large-model  # Only start with --profile large-model

  # GPT-4.1 Proxy (Optional)
  openai-proxy:
    build:
      context: ./docker/proxy
      dockerfile: Dockerfile
    container_name: vllm-openai-proxy
    ports:
      - "8002:8002"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      PROXY_PORT: 8002
      RATE_LIMIT_RPM: ${PROXY_RATE_LIMIT:-100}
      ALLOWED_MODELS: gpt-4-turbo-preview,gpt-4,gpt-3.5-turbo
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - vllm-network
    profiles:
      - proxy  # Only start with --profile proxy

  # Ray Head Node - Orchestration coordinator
  ray-head:
    build:
      context: ./docker/ray
      dockerfile: Dockerfile.head
    container_name: vllm-ray-head
    ports:
      - "8265:8265"  # Ray Dashboard
      - "10001:10001"  # Ray Client
    volumes:
      - ./ray:/app/ray
      - ./coordination:/app/coordination
      - ./memory:/app/memory
      - ray_logs:/tmp/ray
    environment:
      RAY_HEAD: "true"
      RAY_REDIS_ADDRESS: redis:6379
      RAY_OBJECT_STORE_MEMORY: 1000000000
      RAY_DISABLE_IMPORT_WARNING: 1
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY}
      LANGFUSE_HOST: http://langfuse-web:3000
      QDRANT_URL: http://qdrant:6333
      VLLM_PHI_URL: http://vllm-phi:8000
      VLLM_LARGE_URL: http://vllm-large:8001
      OPENAI_PROXY_URL: http://openai-proxy:8002
    depends_on:
      redis:
        condition: service_healthy
      vllm-phi:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "ray", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - vllm-network

  # Ray Worker Nodes
  ray-worker:
    build:
      context: ./docker/ray
      dockerfile: Dockerfile.worker
    volumes:
      - ./ray:/app/ray
      - ./coordination:/app/coordination
      - ./memory:/app/memory
      - ray_logs:/tmp/ray
    environment:
      RAY_HEAD: "false"
      RAY_HEAD_ADDRESS: ray-head:10001
      RAY_REDIS_ADDRESS: redis:6379
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY}
      LANGFUSE_HOST: http://langfuse-web:3000
      QDRANT_URL: http://qdrant:6333
      VLLM_PHI_URL: http://vllm-phi:8000
      VLLM_LARGE_URL: http://vllm-large:8001
      OPENAI_PROXY_URL: http://openai-proxy:8002
    depends_on:
      ray-head:
        condition: service_healthy
    deploy:
      replicas: ${RAY_WORKER_REPLICAS:-2}
    networks:
      - vllm-network

  # Open WebUI (Optional)
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: vllm-open-webui
    ports:
      - "8080:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      OPENAI_API_BASE_URL: http://vllm-phi:8000/v1
      OPENAI_API_KEY: sk-no-key-required
      WEBUI_AUTH: false
      DEFAULT_MODELS: phi-3.5-mini
    depends_on:
      vllm-phi:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - vllm-network
    profiles:
      - webui  # Only start with --profile webui

  # Langflow (Optional) - Visual workflow builder
  langflow:
    image: langflowai/langflow:latest
    container_name: vllm-langflow
    ports:
      - "7860:7860"
    volumes:
      - langflow_data:/app/langflow
    environment:
      LANGFLOW_DATABASE_URL: postgresql://langfuse:${LANGFUSE_DB_PASSWORD:-langfuse123}@langfuse-db:5432/langflow
      LANGFLOW_AUTO_LOGIN: false
    depends_on:
      langfuse-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - vllm-network
    profiles:
      - langflow  # Only start with --profile langflow

networks:
  vllm-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  redis_data:
    driver: local
  clickhouse_data:
    driver: local
  qdrant_data:
    driver: local
  langfuse_db_data:
    driver: local
  vllm_cache:
    driver: local
  ray_logs:
    driver: local
  open_webui_data:
    driver: local
  langflow_data:
    driver: local